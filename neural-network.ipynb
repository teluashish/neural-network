{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMYVTM5peRmT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "# A plain vanilla neural-network where each node (perceptron) in a layer is connected to every other node in the subsequent layer\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "    # Setting the number of nodes in each layer\n",
        "    self.input_nodes = input_nodes\n",
        "    self.hidden_nodes = hidden_nodes\n",
        "    self.output_nodes = output_nodes\n",
        "    # learning_rate controls how far we jump or how much we change weight values so as to not jump around local minima (error) during gradient descent\n",
        "    self.learning_rate = learning_rate\n",
        "    # sigmoid or logistic function is used as a threshold to control or supress output signals from each node if they're not large enough { 1/(1+e^-x) }\n",
        "    # sigmoid outputs are in the range of (0,1) exclusive\n",
        "    self.activation_function = lambda x: expit(x)\n",
        "\n",
        "    # Sampling initial weight matrix values from a normal distribution (from bell curve) with mean = 0, standard deviation = 1/math.sqrt(indegree of a node)\n",
        "    # ih = weights between input and hidden layers, ho = weights between hidden and output layers \n",
        "    # Each row in the weight matrix represents a node in the second layer and each column represents every other node in the first layer\n",
        "    self.weights_ih = np.random.normal(0.0, pow(self.input_nodes, -0.5), (self.hidden_nodes, self.input_nodes))\n",
        "    self.weights_ho = np.random.normal(0.0, pow(self.hidden_nodes, -0.5), (self.output_nodes, self.hidden_nodes))\n",
        "\n",
        "  def feed_forward(self, inputs_list):\n",
        "    # Each column in the inputs_array represent a test sample instance\n",
        "    inputs_array = np.array(inputs_list, ndmin = 2).T\n",
        "\n",
        "    # linear combination of weights and input signals = x\n",
        "    hidden_layer_inputs = np.dot(self.weights_ih, inputs_array)\n",
        "\n",
        "    # x is fed into sigmoid function (present in the node itself) to check if the input signal is stong enough for a node to fire (squish the values)\n",
        "    hidden_layer_outputs = self.activation_function(hidden_layer_inputs)\n",
        "\n",
        "    output_layer_inputs = np.dot(self.weights_ho, hidden_layer_outputs)\n",
        "    final_outputs = output_layer_outputs = self.activation_function(output_layer_inputs)\n",
        "\n",
        "    # outputs from each layer is also returned because we need them during back-propagation\n",
        "    return (inputs_array, hidden_layer_outputs, final_outputs)\n",
        "\n",
        "  def train_weights(self, inputs_list, targets_list):\n",
        "  \n",
        "    inputs_array, hidden_outputs, final_outputs = self.feed_forward(inputs_list)\n",
        "    targets_array = np.array(targets_list, ndmin = 2).T\n",
        "    final_output_errors = targets_array - final_outputs\n",
        "\n",
        "    # (Back-propagation of Errors)\n",
        "    # Each column in the transposed weight matrix represents every other node in the second layer and row represents a node in the first layer\n",
        "    # final_output_errors are in the range of (0,1)\n",
        "    # contribution of the link weights from each node in the first layer is calculated by taking a dot product of (output_errors = proportions) and each row in the transposed weights matrix\n",
        "    hidden_layer_errors = np.dot(self.weights_ho.T, final_output_errors)\n",
        "    \n",
        "    # (Applying Gradient Descent)\n",
        "    # Update the weights controlled by learning rate and changed by the slope or derivative of the error cost function (y = errors, x = all contributing weights)\n",
        "    self.weights_ho += self.learning_rate * np.dot((final_output_errors * final_outputs * (1.0 - final_outputs)), hidden_outputs.T)\n",
        "    self.weights_ih += self.learning_rate * np.dot((hidden_layer_errors * hidden_outputs * (1.0 - hidden_outputs)), inputs_array.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.ndimage\n",
        "%matplotlib inline\n",
        "\n",
        "def fit_transform(ob, isInput = True):\n",
        "  if isInput:\n",
        "    return np.asfarray(ob)/255.0 * 0.99 + 0.01 \n",
        "  else:\n",
        "    output = np.zeros(10) + 0.01\n",
        "    output[ob] = 0.99\n",
        "    return output\n",
        "\n",
        "def train(instance, train_input, train_label):\n",
        "  for idx in range(len(train_inputs)):\n",
        "    train_input = fit_transform(train_inputs[idx])\n",
        "    train_label = fit_transform(train_labels[idx], isInput = False)\n",
        "    instance.train_weights(train_input, train_label)\n",
        "\n",
        "    plusx_train_input = scipy.ndimage.interpolation.rotate(train_input.reshape(28,28), 10, cval=0.01, order=1, reshape=False)\n",
        "    instance.train_weights(plusx_train_input.reshape(784), train_label)\n",
        "\n",
        "    minusx_train_input = scipy.ndimage.interpolation.rotate(train_input.reshape(28,28), -10, cval=0.01, order=1, reshape=False)\n",
        "    instance.train_weights(minusx_train_input.reshape(784), train_label)\n",
        "\n",
        "def accuracy(instance, test_file_name):\n",
        "  test_df = pd.read_csv(test_file_name)\n",
        "  test_inputs, test_labels = test_df.drop(\"label\", axis = 1).to_numpy(), test_df[\"label\"].to_numpy()\n",
        "  true_count = 0\n",
        "  for idx in range(len(test_inputs)):\n",
        "    (inputs_array, hidden_layer_outputs, final_outputs) = instance.feed_forward(fit_transform(test_inputs[idx]))\n",
        "    true_count += 1 if np.argmax(final_outputs) == test_labels[idx] else 0\n",
        "  return true_count / len(test_inputs)\n",
        "\n",
        "instance = NeuralNetwork(784, 200, 10, 0.2)\n",
        "train_df = pd.read_csv(\"mnist_train.csv\")\n",
        "train_inputs, train_labels = train_df.drop(\"label\", axis = 1).to_numpy(), train_df[\"label\"].to_numpy()\n",
        "\n",
        "epochs = 7\n",
        "for _ in range(epochs):\n",
        "  train(instance, train_inputs, train_labels)\n",
        "print(accuracy(instance, \"mnist_test.csv\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eDsikz0eJ_hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61b127e-70d1-4a5a-e816-a2b3f28d96fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9639\n"
          ]
        }
      ]
    }
  ]
}